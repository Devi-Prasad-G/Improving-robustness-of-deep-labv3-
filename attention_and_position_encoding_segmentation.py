# -*- coding: utf-8 -*-
"""Attention and position encoding for submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EL0TET-v84buI7-Kj_dwsfw9h1NCN-hX
"""

#from preprocessing import *
import preprocessing
import tensorflow as tf
import os
import cv2
from keras.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
import tensorflow.keras.backend as K
import random
from random import choice


from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Dropout, Conv2DTranspose, Cropping2D, Add, UpSampling2D, LayerNormalization, BatchNormalization

from os import path
from matplotlib import pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras import activations

import keras as k
import PIL
from PIL import Image
from tensorflow.keras.utils import to_categorical
import shutil
from keras.preprocessing.image import ImageDataGenerator
#tf.config.experimental_run_functions_eagerly(True)

class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):
  def __init__(self,
               y_true=None,
               y_pred=None,
               num_classes=None,
               name=None,
               dtype=None,**kwargs):
    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype,**kwargs)

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_pred = tf.math.argmax(y_pred, axis=-1)
    return super().update_state(y_true, y_pred, sample_weight)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "num_classes": self.num_classes}

import matplotlib.pyplot as plt

from tensorflow.keras.utils import to_categorical
def createClassMap(image, yseg):
    y_pool = tf.one_hot(yseg, depth=21)
    y_pool = tf.image_resize(y_pool, size=(image.shape[1]//4,image.shape[2]//4))
    y_pool = tf.squeeze(y_pool,axis=2)
    return image, y_pool

def add_l2_regularization_kernel(layer, weight):
    def _add_l2_regularization_kernel():
        l2 = tf.keras.regularizers.l2(weight)
        return l2(layer.kernel)
    return _add_l2_regularization_kernel

def add_l2_regularization_bias(layer, weight):
    def _add_l2_regularization_bias():
        l2 = tf.keras.regularizers.l2(weight)
        return l2(layer.bias)
    return _add_l2_regularization_bias

def patch_encoding(patches,dim):
  positions = tf.range(start=0, limit=patches.shape[1]*patches.shape[2], delta=1)
  position_embedding = layers.Embedding(
      input_dim=dim*dim, output_dim=128
  )(positions)
  position_embedding = tf.reshape(position_embedding, [-1,dim,dim,128])
  patches = patches + position_embedding
  return patches

import keras
def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = tf.keras.layers.Dense(units, activation=tf.nn.relu)(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
    return x

def convolution_block(
    block_input,
    num_filters=256,
    kernel_size=3,
    dilation_rate=1,
    padding="same",
    use_bias=False,
):
    x = layers.Conv2D(
        num_filters,
        kernel_size=kernel_size,
        dilation_rate=dilation_rate,
        padding="same",
        use_bias=use_bias,
        kernel_initializer=keras.initializers.HeNormal(),
    )(block_input)
    x = layers.BatchNormalization()(x)
    return tf.nn.relu(x)


#creating two models, one for pixel level prediction, another for patch level object prediction for each 16*16 patch
def AttentionPlusClassPrediction(image_size, num_classes):
    model_input = keras.Input(shape=(image_size, image_size, 3))
    resnet101 = tf.keras.applications.Reset101(
        weights="imagenet", include_top=False, input_tensor=model_input
    )
    x1 = resnet101.get_layer("conv4_block23_2_relu").output
    input_b = resnet101.get_layer("conv2_block3_2_relu").output

    #Attention Layer
    average_b = layers.Conv2D(128,kernel_size=(4,4),strides=(4,4),padding='same')(input_b)
    average = layers.Concatenate()([x1,average_b])
    atconv = Conv2D(128, (1, 1), activation='relu', padding='same')(average)
    encoded = patch_encoding(atconv,24)
    attend = tf.keras.layers.MultiHeadAttention(num_heads=4,key_dim=128)(key=encoded,value=encoded,query=encoded)
    attend = layers.LayerNormalization()(attend)
    attend = layers.Add()([attend,atconv])
    ml=mlp(attend, [128], 0.3)
    ml=layers.LayerNormalization()(ml)
    ml = layers.Add()([ml,attend])

    attend = tf.keras.layers.MultiHeadAttention(num_heads=4,key_dim=128)(key=ml,value=ml,query=ml)
    attend = layers.LayerNormalization()(attend)
    ml2=mlp(attend, [128], 0.3)
    ml2=layers.LayerNormalization()(ml2)
    ml2 = layers.Add()([ml2,attend])

    conc = layers.Concatenate(axis=-1)([x1,ml2])
    
    #branch for class_prediction
    clas_pred = layers.Conv2D(256, kernel_size=1, padding="same",use_bias=True,
        kernel_initializer=keras.initializers.HeNormal(),
    )(conc)
    clas_pred = layers.BatchNormalization()(clas_pred)
    clas_pred = layers.Conv2D(21, kernel_size=1, padding="same",use_bias=True,
        kernel_initializer=keras.initializers.HeNormal(), activation="sigmoid"
    )(clas_pred)
    
    #clas_pred_up is the upsampled class predictions to be concatenated with the upsampled predictions of the decoder branch
    clas_pred_up = layers.UpSampling2D(size=(image_size // 4 //  clas_pred.shape[1], image_size // 4// clas_pred.shape[2]),interpolation="bilinear")(clas_pred)
      
    
    #upsampling the results of attention layer
    input_a = layers.UpSampling2D(
        size=(image_size // 4 // conc.shape[1], image_size // 4 // conc.shape[2]),
        interpolation="bilinear",
    )(conc)
    
    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)
    x = layers.Concatenate(axis=-1)([input_a, input_b, clas_pred_fin])
    x = convolution_block(x, kernel_size=(3,3))

    #final upsampling to original image size
    x = layers.UpSampling2D(
        size=(image_size // x.shape[1], image_size // x.shape[2]),
        interpolation="bilinear",
    )(x)
    x = layers.Conv2D(64, kernel_size=(3, 3), padding="same", activation='relu')(x)
    
    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding="same", activation='softmax')(x)
    
    return keras.Model(inputs=model_input, outputs=model_output), keras.Model(inputs=model_input, outputs=clas_pred)


_MIN_SCALE = 0.5
_MAX_SCALE = 2.0
_IGNORE_LABEL = 255
IMAGE_SIZE = 384

#checkpoints for saving models
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath="/content/drive/MyDrive/mlp_cw4/attention_iter.h5",
    monitor= "val_updated_mean_io_u",
    mode='max',
    save_best_only=True,period=40)

checkpoint2 = tf.keras.callbacks.ModelCheckpoint(
    filepath="/content/drive/MyDrive/mlp_cw4/class_iter.h5",
    monitor= "val_categorical_accuracy",
    mode='max',
    save_best_only=True)

def get_filenames(is_training, data_dir):
  """Return a list of filenames.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: path to the the directory containing the input data.
  Returns:
    A list of file names.
  """
  if is_training:
    return [os.path.join(data_dir, 'voc_train1.record')]
  else:
    return [os.path.join(data_dir, 'voc_val.record')]

def parse_record(raw_record):
  """Parse PASCAL image and label from a tf record."""
  keys_to_features = {
      'image/height':
      tf.io.FixedLenFeature((), tf.int64),
      'image/width':
      tf.io.FixedLenFeature((), tf.int64),
      'image/encoded':
      tf.io.FixedLenFeature((), tf.string, default_value=''),
      'image/format':
      tf.io.FixedLenFeature((), tf.string, default_value='jpeg'),
      'label/encoded':
      tf.io.FixedLenFeature((), tf.string, default_value=''),
      'label/format':
      tf.io.FixedLenFeature((), tf.string, default_value='png'),
  }

  parsed = tf.io.parse_single_example(raw_record, keys_to_features)

  image = tf.io.decode_image(
      tf.reshape(parsed['image/encoded'], shape=[]), 3)
  image = tf.cast(tf.image.convert_image_dtype(image, dtype=tf.uint8),dtype=tf.float32)

  image.set_shape([None, None, 3])

  label = tf.image.decode_png(
      tf.reshape(parsed['label/encoded'], shape=[]), 1)
  label = tf.cast(tf.image.convert_image_dtype(label, dtype=tf.uint8),dtype=tf.int32)
  label.set_shape([None, None, 1])

  return image, label

def preprocess_image(image, label, is_training):
  """Preprocess a single image of layout [height, width, depth]."""
  if is_training:
    # Randomly scale the image and label.
    image, label = preprocessing.random_rescale_image_and_label(
        image, label, _MIN_SCALE, _MAX_SCALE)

    # Randomly crop or pad a [_HEIGHT, _WIDTH] section of the image and label.
    image, label = preprocessing.random_crop_or_pad_image_and_label(
        image, label, IMAGE_SIZE, IMAGE_SIZE, _IGNORE_LABEL)

    # Randomly flip the image and label horizontally.
    image, label = preprocessing.random_flip_left_right_image_and_label(
        image, label)
    
    
    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])
    label.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])
    
  image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
  label = tf.image.resize(images=label, size=[IMAGE_SIZE, IMAGE_SIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    
  image = preprocessing.mean_image_subtraction(image)
  label = tf.cast(label, tf.int32)
  label = tf.where(label==255, 0, label)
  return image, label


def input_fn(is_training, data_dir, batch_size, model, num_epochs=1):
  """Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: The directory containing the input data.
    batch_size: The number of samples per batch.
    num_epochs: The number of epochs to repeat the dataset.
  Returns:
    A tuple of images and labels.
  """
  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, '/content/drive/MyDrive'))
  dataset = dataset.flat_map(tf.data.TFRecordDataset)

  if is_training:
    # When choosing shuffle buffer sizes, larger sizes result in better
    # randomness, while smaller sizes have better performance.
    # is a relatively small dataset, we choose to shuffle the full epoch.
    dataset = dataset.shuffle(buffer_size=10582)

  dataset = dataset.map(parse_record)
  dataset = dataset.map(
          lambda image, label: preprocess_image(image, label, is_training))
  
  dataset3 = dataset.map(
          lambda image, label: createClassMap(image, label))
    # We call repeat after shuffling, rather than before, to prevent separate
    # epochs from blending together.

  dataset = dataset.batch(batch_size,drop_remainder=True)
  dataset = dataset.repeat()
  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
  
  dataset3 = dataset3.batch(batch_size,drop_remainder=True)
  dataset3 = dataset3.repeat()
  dataset3 = dataset3.prefetch(tf.data.experimental.AUTOTUNE)

    
  return dataset, dataset3


starter_learning_rate = 0.006
decay_steps = 54000
end_learning_rate = 3e-6
learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(
    starter_learning_rate,
    decay_steps,
    end_learning_rate,
    power=0.9)

starter_learning_rate2 = 0.006
learning_rate_fn2 = tf.keras.optimizers.schedules.PolynomialDecay(
    starter_learning_rate2,
    decay_steps,
    end_learning_rate,
    power=0.9)

opt = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)
opt2 = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn2, momentum=0.9)

metrics = [UpdatedMeanIoU(num_classes=21),'sparse_categorical_accuracy']

loss = keras.losses.SparseCategoricalCrossentropy()

model, model_class = AttentionPlusClassPrediction(image_size=IMAGE_SIZE, num_classes=21)

loss_class = keras.losses.CategoricalCrossentropy()
loss = keras.losses.SparseCategoricalCrossentropy()

metrics2 = ['categorical_accuracy']
model.compile(
    optimizer=opt2,
    loss=loss,
    metrics=metrics,
    run_eagerly=True
)

model_class.compile(
    optimizer=opt,
    loss=loss1,
    metrics=metrics2,
    run_eagerly=True
)

alpha = 0.0001  # weight decay coefficient

for layer in model.layers:
    if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):
        layer.add_loss(add_l2_regularization_kernel(layer, alpha))
    if hasattr(layer, 'bias_regularizer') and layer.use_bias:
        layer.add_loss(add_l2_regularization_bias(layer, alpha))

for i in range(10):
    train_dataset, class_dataset = input_fn(True,'/content/drive/MyDrive', 8, model, 4)
    val_set, class_val_dataset = input_fn(False,'/content/drive/MyDrive', 8, model, 4)

    model.fit(train_dataset, validation_data=val_set, epochs=5500,steps_per_epoch=1,validation_steps=1,verbose=2,callbacks=[checkpoint])
    model_class.fit(class_dataset, validation_data=class_val_dataset, epochs=1000,steps_per_epoch=1,validation_steps=1,verbose=2,callbacks=[checkpoint2])