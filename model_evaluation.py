# -*- coding: utf-8 -*-
"""Model Evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6YMcS26WkkSMQyfQl4UOqDD_g_5GtZu
"""

import tensorflow as tf
import tensorflow_addons as tfa
from PIL import Image
import numpy as np
from matplotlib import pyplot as plt
from skimage import transform
import preprocessing
import os

import preprocessing

"""## Baseline experiments"""

class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):
  def __init__(self,
               y_true=None,
               y_pred=None,
               num_classes=None,
               name=None,
               dtype=None,**kwargs):
    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype,**kwargs)

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_pred = tf.math.argmax(y_pred, axis=-1)
    return super().update_state(y_true, y_pred, sample_weight)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "num_classes": self.num_classes}

model = tf.keras.models.load_model("/content/drive/MyDrive/mlp_cw4/Baseline",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

model = tf.keras.models.load_model("/content/drive/MyDrive/Baseline.h5",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

model_cont = tf.keras.models.load_model("/content/drive/MyDrive/mlp_cw4/cont_atten+patch_ver2.h5",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

IMAGE_SIZE = 384

def load(filename):
   np_image = Image.open(filename)
   np_image = np.array(np_image)
   return np_image

def preprocess_image(image):  
  image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
  image = preprocessing.mean_image_subtraction(image)
  return image

def cutout(image,mask_size,offset,constant_values =0):
  image = tfa.image.cutout(image,mask_size,offset,constant_values)
  return image

def blur(image,std,constant_values =0):
  image = image + tf.random.normal(shape=image.shape, stddev=std)
  return image

filename = "/content/drive/MyDrive/2012_002445.jpg"
#filename = "/content/drive/MyDrive/mlp_cw4_images/2007_002105.jpg"
np_image = load(filename)
image = preprocess_image(np_image)

plt.imshow(image)
plt.show()

image1 = tf.expand_dims(image,axis =0)
pred = model.predict(image1)

print(pred.shape)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

"""blur"""

mask_size = (40,40)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = blur(image_c,6,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = model.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

mask_size = (40,40)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = blur(image_c,6,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = model_cont.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

mask_size = (40,40)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = blur(image_c,4.77,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = model.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

"""cutout of 80,80"""

mask_size = (40,40)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = cutout(image_c,mask_size,offset,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = model.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

mask_size = (80,80)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = cutout(image_c,mask_size,offset,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = model.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

mask_size = (40,40)
offset = (150, 200)
constant_values = 0
image_c1 = tf.expand_dims(image, axis=0)
image_c1 = cutout(image_c,mask_size,offset,constant_values)
print(image_c1.shape)
plt.imshow(np.array(image_c1[0]))
plt.show()
pred1 = model.predict(image_c1)
pred1 = tf.math.argmax(pred1[0],axis = 2)
plt.imshow(pred1)
plt.show()

mask_size = (120,120)
offset = (150, 200)
constant_values = 0
image_c1 = tf.expand_dims(image, axis=0)
image_c1 = cutout(image_c,mask_size,offset,constant_values)
print(image_c1.shape)
plt.imshow(np.array(image_c1[0]))
plt.show()
pred1 = model.predict(image_c1)
pred1 = tf.math.argmax(pred1[0],axis = 2)
plt.imshow(pred1)
plt.show()

"""# Attention_model experiments"""

model_att = tf.keras.models.load_model("/content/drive/MyDrive/Attention_pos_model",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

mask_size = (120,120)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = cutout(image_c,mask_size,offset,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = cont.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

mask_size = (120,120)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
#image_c = cutout(image_c,mask_size,offset,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = model_att.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

_MIN_SCALE = 0.5
_MAX_SCALE = 2.0
_IGNORE_LABEL = 255
IMAGE_SIZE = 384


def get_filenames(is_training, data_dir):
  """Return a list of filenames.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: path to the the directory containing the input data.
  Returns:
    A list of file names.
  """
  if is_training:
    return [os.path.join(data_dir, 'voc_train1.record')]
  else:
    return [os.path.join(data_dir, 'voc_val.record')] #'voc_val (1).record'

def parse_record(raw_record):
  """Parse PASCAL image and label from a tf record."""
  keys_to_features = {
      'image/height':
      tf.io.FixedLenFeature((), tf.int64),
      'image/width':
      tf.io.FixedLenFeature((), tf.int64),
      'image/encoded':
      tf.io.FixedLenFeature((), tf.string, default_value=''),
      'image/format':
      tf.io.FixedLenFeature((), tf.string, default_value='jpeg'),
      'label/encoded':
      tf.io.FixedLenFeature((), tf.string, default_value=''),
      'label/format':
      tf.io.FixedLenFeature((), tf.string, default_value='png'),
  }

  parsed = tf.io.parse_single_example(raw_record, keys_to_features)
  # height = tf.cast(parsed['image/height'], tf.int32)
  # width = tf.cast(parsed['image/width'], tf.int32)

  image = tf.io.decode_image(
      tf.reshape(parsed['image/encoded'], shape=[]), 3)
  #image = tf.cast(tf.image.convert_image_dtype(image, dtype=tf.uint8),dtype=tf.float32)
  #plt.imshow(image[0].numpy().astype('int32'))
  #plt.show()

  image.set_shape([None, None, 3])

  label = tf.image.decode_png(
      tf.reshape(parsed['label/encoded'], shape=[]), 1)
  label = tf.cast(tf.image.convert_image_dtype(label, dtype=tf.uint8),dtype=tf.int32)
  label.set_shape([None, None, 1])

  return image, label

def preprocess_image(image, label, is_training):
  """Preprocess a single image of layout [height, width, depth]."""
  if is_training:
    # Randomly scale the image and label.
    image, label = preprocessing.random_rescale_image_and_label(
        image, label, _MIN_SCALE, _MAX_SCALE)

    # Randomly crop or pad a [_HEIGHT, _WIDTH] section of the image and label.
    image, label = preprocessing.random_crop_or_pad_image_and_label(
        image, label, IMAGE_SIZE, IMAGE_SIZE, _IGNORE_LABEL)

    # Randomly flip the image and label horizontally.
    image, label = preprocessing.random_flip_left_right_image_and_label(
        image, label)
    
    
    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])
    label.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])
    
  image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
  label = tf.image.resize(images=label, size=[IMAGE_SIZE, IMAGE_SIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    
  #image = preprocessing.mean_image_subtraction(image)
  label = tf.cast(label, tf.int32)
  label = tf.where(label==255, 0, label)
  return image, label

@tf.function
def get_box(lambda_value,img_size,rx,ry):
    IMG_SIZE = img_size

    cut_rat = 0.2

    cut_w = IMG_SIZE * cut_rat  # rw
    cut_w = tf.cast(cut_w, tf.int32)

    cut_h = IMG_SIZE * cut_rat  # rh
    cut_h = tf.cast(cut_h, tf.int32)

    cut_x = rx #tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx
    cut_y = ry #tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry
    #print(cut_x,cut_y,cut_x.shape,cut_y.shape,type(cut_x),type(cut_y))

    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)
    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)
    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)
    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)

    target_h = bby2 - boundaryy1
    if target_h == 0:
        target_h += 1

    target_w = bbx2 - boundaryx1
    if target_w == 0:
        target_w += 1

    return boundaryx1, boundaryy1, target_h, target_w

def cutout(image, label, size=384):
    #(image, label) = train_ds_one
    #plt.imshow(image.numpy())
    #plt.show()
    IMG_SIZE=size

    alpha = [0.25]
    beta = [0.25]

    # Get a sample from the Beta distribution
    #lambda_value = sample_beta_distribution(1, alpha, beta)

    # Define Lambda
    lambda_value = 0.2
    #label1 = np.array(label)
    #cood = np.transpose(np.nonzero(label1))
    #print("label shape in cutout function = ",label.shape)
    #print(label)
    zero = tf.constant(0, dtype=tf.int32)
    label_n = tf.reshape(label,shape = (384,384))
    #print("after ",label_n.shape)
    #max = tf.reduce_max(tf.slice(tf.unique_with_counts(tf.reshape(label,[-1]))[2],[1],[-1]))
    where = tf.not_equal(label_n, zero)
    #print(where)     
    cood = tf.where(where)[10] #taking first coordinate
    #print(cood.numpy())
    #print("HI")
    x,y = tf.reshape(cood[0],shape = (1,)), tf.reshape(cood[1],shape=(1,)) #converting to correct shape
    rx,ry = tf.cast(x,dtype = tf.int32),tf.cast(y,dtype = tf.int32) #converting to correct dtype
    #print(rx,ry)
    # Get the bounding box offsets, heights and widths
    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value, size,rx,ry)
    boundaryx2, boundaryy2, target_h2, target_w2 = get_box(lambda_value, label.shape[1],rx,ry)

    # Get a patch from the second image (`image2`)
    crop2 = tf.image.crop_to_bounding_box(
        image, boundaryy1, boundaryx1, target_h, target_w
    )

    cropl = tf.image.crop_to_bounding_box(
        label, boundaryy1, boundaryx1, target_h, target_w
    )
    # Pad the `image2` patch (`crop2`) with the same offset
    image1 = tf.image.pad_to_bounding_box(
        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE
    )

    image = image-image1

    label1 = tf.image.pad_to_bounding_box(
        cropl, boundaryy1, boundaryx1, label.shape[1], label.shape[1]
    )

    label = label-label1
    #print(type(image),image.shape())
    #plt.imshow(image.numpy())
    #plt.show()
    # Modify the first image by subtracting the patch from `image1`
    # (before applying the `image2` patch)
    # Add the modified `image1` and `image2`  together to get the CutMix image

    # Adjust Lambda in accordance to the pixel ration

    # Combine the labels of both images
    #label = lambda_value * label1 + (1 - lambda_value) * label2
    return image, label

def input_fn_pert(is_training, data_dir, batch_size, model, num_epochs=1):
  """Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: The directory containing the input data.
    batch_size: The number of samples per batch.
    num_epochs: The number of epochs to repeat the dataset.
  Returns:
    A tuple of images and labels.
  """
  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(False, '/content/drive/MyDrive'))
  dataset = dataset.flat_map(tf.data.TFRecordDataset)

  if is_training:
    # When choosing shuffle buffer sizes, larger sizes result in better
    # randomness, while smaller sizes have better performance.
    # is a relatively small dataset, we choose to shuffle the full epoch.
    dataset = dataset.shuffle(buffer_size=10582)

  dataset = dataset.map(parse_record)
  print("before the preprocess")

  dataset = dataset.map(
          lambda image, label: preprocess_image(image, label, is_training))

  print("after the preprocess")

  dataset2 = dataset.map(
          lambda image, label: cutout(image, label))    # rescaling is done here
  
  dataset = dataset.batch(batch_size,drop_remainder=True)
  #dataset = dataset.repeat()

  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

  dataset2 = dataset2.batch(batch_size,drop_remainder=True)
  #dataset = dataset.repeat()    
  dataset2 = dataset2.prefetch(tf.data.experimental.AUTOTUNE)
  
  n=0
  miou_normal = 0
  miou_pert = 0

  orig_miou = UpdatedMeanIoU(num_classes=21)
  orig_miou_pert = UpdatedMeanIoU(num_classes=21)
  for image, label in dataset:
    preds = model.predict(image, steps=1)
    #miou_n = UpdatedMeanIoU(num_classes=21)
    #miou_n.update_state(label,preds)
    orig_miou.update_state(label,preds)
    n+=1

  for image, label in dataset2:
    preds2 = model.predict(image, steps=1)
    #miou_pert = UpdatedMeanIoU(num_classes=21)
    #miou_n.update_state(label,preds)
    orig_miou_pert.update_state(label,preds2)
  
  print(orig_miou.result().numpy())
  print(orig_miou_pert.result().numpy())
  return dataset, dataset2

"""## Attention_model_results"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, model_att, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, model_att, 4)

"""CUT_RAT 0.1

"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, model_att, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, model_att, 4)

"""CUT_RAT 0.3"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, model_att, 4)

"""## Baseline_model_results"""

baseline = tf.keras.models.load_model("/content/drive/MyDrive/Baseline.h5",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

"""CUT ratio 0.1"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, baseline, 4)

"""CUT rat 0.2"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, baseline, 4)

"""0.3"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, baseline, 4)

"""CONTOUR MODEL"""

cont = tf.keras.models.load_model("/content/drive/MyDrive/cont_atten+patch.h5",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

"""CUT_RAT 0.1"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, cont, 4)

"""0.2"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, cont, 4)

"""0.3"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, cont, 4)

mask_size = (120,120)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = cutout(image_c,mask_size,offset,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = cont.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

"""Deformable Cutout"""

defor = tf.keras.models.load_model("/content/drive/MyDrive/Sol1(deformable_conv).h5",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

"""Deformable Gaussian"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 30, defor, 4)

mask_size = (40,40)
offset = (150, 200)
constant_values = 0
image_c = tf.expand_dims(image, axis=0)
image_c = cutout(image_c,mask_size,offset,constant_values)
print(image_c.shape)
plt.imshow(np.array(image_c[0]))
plt.show()
pred = cont.predict(image_c)
pred = tf.math.argmax(pred[0],axis = 2)
plt.imshow(pred)
plt.show()

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 10, cont, 4)

"""GAUSSIAN NOISE RESULTS"""

deform = tf.keras.models.load_model("/content/drive/MyDrive/mlp_cw4/Deformable",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

def gaussian(image, label):
  print(image.shape)
  image = image + tf.random.normal(shape=image.shape, stddev=8)
  print(image.shape)
  return image, label

def input_fn_pert(is_training, data_dir, batch_size, model, num_epochs=1):
  """Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: The directory containing the input data.
    batch_size: The number of samples per batch.
    num_epochs: The number of epochs to repeat the dataset.
  Returns:
    A tuple of images and labels.
  """
  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(False, '/content/drive/MyDrive'))
  dataset = dataset.flat_map(tf.data.TFRecordDataset)

  if is_training:
    # When choosing shuffle buffer sizes, larger sizes result in better
    # randomness, while smaller sizes have better performance.
    # is a relatively small dataset, we choose to shuffle the full epoch.
    dataset = dataset.shuffle(buffer_size=10582)

  dataset = dataset.map(parse_record)
  print("before the preprocess")

  dataset = dataset.map(
          lambda image, label: preprocess_image(image, label, is_training))

  print("after the preprocess")

  dataset2 = dataset
  #dataset2 = dataset.map(
  #        lambda image, label: gaussian(image, label))    # rescaling is done here
  
  dataset = dataset.batch(batch_size,drop_remainder=True)
  #dataset = dataset.repeat()

  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

  dataset2 = dataset2.batch(batch_size,drop_remainder=True)
  #dataset = dataset.repeat()    
  dataset2 = dataset2.prefetch(tf.data.experimental.AUTOTUNE)
  
  n=0
  miou_normal = 0
  miou_pert = 0

  orig_miou = UpdatedMeanIoU(num_classes=21)
  orig_miou_pert = UpdatedMeanIoU(num_classes=21)
  '''for image, label in dataset:
    preds = model.predict(image, steps=1)
    
    orig_miou.update_state(label,preds)
    n+=1'''

  for (image1, label1),(image2,label2) in zip(dataset,dataset2):
    #preds2 = model.predict(image2, steps=1)
    #miou_pert = UpdatedMeanIoU(num_classes=21)
    #miou_n.update_state(label,preds)
    miou_n = UpdatedMeanIoU(num_classes=21)
    #miou_n.update_state(label2,preds2)
    #print(miou_n.result().numpy())
    #if(miou_n.result().numpy()>0.7):
      #pred = model.predict(image1)
    plt.imshow(image1[0])
    plt.show()
      #plt.imshow(tf.math.argmax(pred[0],axis = 2))
      #plt.show()
      #predb = baseline.predict(image1)
      #plt.imshow(tf.math.argmax(predb[0],axis = 2))
      #plt.show()

    #orig_miou_pert.update_state(label2,preds2)
  
  #print(orig_miou.result().numpy())
  #print(orig_miou_pert.result().numpy())
  return dataset, dataset2

#@title Default title text
val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

"""4.77"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, baseline, 4)

"""0.6"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, baseline, 4)

"""0.8"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, baseline, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

"""0.6 attention"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

"""0.8 """

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

"""contour

"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

"""deformable"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, deform, 4) # std = 4

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, deform, 4) #std 6

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, deform, 4) #std 8

"""RESCALING"""

def resize_pad(image, label):
  image = tf.image.crop_and_resize(tf.expand_dims(image,0), boxes=[[0.3, 0.3, 0.7,0.7]],box_indices=[0], crop_size=(384,384))
  label = tf.image.crop_and_resize(tf.expand_dims(label,0), boxes=[[0.3, 0.3, 0.7,0.7]],box_indices=[0], crop_size=(384,384))
  return tf.squeeze(image,0), tf.squeeze(label,0)

def input_fn_pert(is_training, data_dir, batch_size, model, num_epochs=1):
  """Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: The directory containing the input data.
    batch_size: The number of samples per batch.
    num_epochs: The number of epochs to repeat the dataset.
  Returns:
    A tuple of images and labels.
  """
  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(False, '/content/drive/MyDrive'))
  dataset = dataset.flat_map(tf.data.TFRecordDataset)

  if is_training:
    # When choosing shuffle buffer sizes, larger sizes result in better
    # randomness, while smaller sizes have better performance.
    # is a relatively small dataset, we choose to shuffle the full epoch.
    dataset = dataset.shuffle(buffer_size=10582)

  dataset = dataset.map(parse_record)
  print("before the preprocess")

  dataset = dataset.map(
          lambda image, label: preprocess_image(image, label, is_training))

  print("after the preprocess")

  dataset2 = dataset.map(
          lambda image, label: resize_pad(image, label))    # rescaling is done here
  
  dataset = dataset.batch(batch_size,drop_remainder=True)
  #dataset = dataset.repeat()

  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

  dataset2 = dataset2.batch(batch_size,drop_remainder=True)
  #dataset = dataset.repeat()    
  dataset2 = dataset2.prefetch(tf.data.experimental.AUTOTUNE)
  
  n=0
  miou_normal = 0
  miou_pert = 0

  orig_miou = UpdatedMeanIoU(num_classes=21)
  orig_miou_pert = UpdatedMeanIoU(num_classes=21)
  for image, label in dataset:
    preds = model.predict(image, steps=1)
    orig_miou.update_state(label,preds)
    n+=1

  for image, label in dataset2:
    preds2 = model.predict(image, steps=1)

    orig_miou_pert.update_state(label,preds2)
  
  print(orig_miou.result().numpy())
  print(orig_miou_pert.result().numpy())
  return dataset, dataset2

"""Defor"""

defor = tf.keras.models.load_model("/content/drive/MyDrive/Sol1(deformable_conv).h5",custom_objects={'UpdatedMeanIoU':UpdatedMeanIoU})

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, defor, 4)

"""scaling comparison deformable and baseline 0.1-0.6"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

"""scaling 0.2-0.8"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

"""scaling 0.3-0.7"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, defor, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, model_att, 4)

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

"""Contour"""

val_set, val_set_pert = input_fn_pert(False,'/content/drive/MyDrive', 20, cont, 4)

"""graphs for experiments"""

from matplotlib import pyplot as plt
labels = ["baseline","contour","deformable","attention"]
clean_images = [72.99,72.92,73.16,73.74]
cut_rat_1 = [72.43,72.19,72.47,72.95]
cut_rat_2 = [71.62,71.14,71.72,71.82]
cut_rat_3 = [69.70,70.11,70.16,70.96]
plt.plot(labels,clean_images,color='black',label = 'clean images')
plt.plot(labels,cut_rat_1,color = 'red',label= 'cut ratio=0.1')
plt.plot(labels,cut_rat_2,color = 'blue',label= 'cut ratio=0.2')
plt.plot(labels,cut_rat_3,color = 'green',label= 'cut ratio=0.3')
plt.xlabel("model archtectures")
plt.ylabel("mIOU in percentage")
plt.grid()
plt.legend()
plt.savefig("/content/drive/MyDrive/mlp_cw4_images/cutout.pdf")
plt.show()

labels = ["baseline","contour","deformable","attention"]
clean_images = [72.99,72.92,73.16,73.74]
gauss_1 = [72.27,72.3,72.75,73.34]
gauss_2 = [71.62,72.01,72.43,73.13]
gauss_3 = [70.64,71.17,71.45,72.26]
plt.plot(labels,clean_images,color = 'black',label = 'clean images')
plt.plot(labels,gauss_1,color = 'red',label= 'gauss_blur_std = 4')
plt.plot(labels,gauss_2,color = 'blue',label= 'gauss_blur_std = 6')
plt.plot(labels,gauss_3,color = 'green',label= 'gauss_blur_std = 8')
plt.xlabel("model archtectures")
plt.ylabel("mIOU in percentage")
plt.grid()
plt.legend()
plt.savefig("/content/drive/MyDrive/mlp_cw4_images/gauss_blur.pdf")
plt.show()