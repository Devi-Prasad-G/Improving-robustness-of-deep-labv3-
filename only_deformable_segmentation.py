# -*- coding: utf-8 -*-
"""Only Deformable Submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10jh03K2NhfxvBcKXdRQDxtv1hF9qg2E6
"""

import preprocessing
import tensorflow as tf
import os
import cv2
from keras.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
import tensorflow.keras.backend as K
import random
from random import choice
from keras.layers import Conv2D, MaxPooling2D, Input, ZeroPadding2D, Dropout, Conv2DTranspose, Cropping2D, Add, UpSampling2D, LayerNormalization, BatchNormalization

from os import path
from matplotlib import pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras import activations

import keras as k
import PIL
from PIL import Image
from tensorflow.keras.utils import to_categorical
import shutil
from keras.preprocessing.image import ImageDataGenerator

import matplotlib.pyplot as plt

#updated miou is a wrapper over miou for sparse cross enropy (single class prediction from multi)
class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):
  def __init__(self,
               y_true=None,
               y_pred=None,
               num_classes=None,
               name=None,
               dtype=None,**kwargs):
    super(UpdatedMeanIoU, self).__init__(num_classes = num_classes,name=name, dtype=dtype,**kwargs)

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_pred = tf.math.argmax(y_pred, axis=-1)
    return super().update_state(y_true, y_pred, sample_weight)

  def get_config(self):
    base_config = super().get_config()
    return {**base_config, "num_classes": self.num_classes}

def add_l2_regularization_kernel(layer, weight):
    def _add_l2_regularization_kernel():
        l2 = tf.keras.regularizers.l2(weight)
        return l2(layer.kernel)
    return _add_l2_regularization_kernel

def add_l2_regularization_bias(layer, weight):
    def _add_l2_regularization_bias():
        l2 = tf.keras.regularizers.l2(weight)
        return l2(layer.bias)
    return _add_l2_regularization_bias

import keras
def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = tf.keras.layers.Dense(units, activation=tf.nn.relu)(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
    return x

def convolution_block(
    block_input,
    num_filters=256,
    kernel_size=3,
    dilation_rate=1,
    padding="same",
    use_bias=False,
):
    x = layers.Conv2D(
        num_filters,
        kernel_size=kernel_size,
        dilation_rate=dilation_rate,
        padding="same",
        use_bias=use_bias,
        kernel_initializer=keras.initializers.HeNormal(),
    )(block_input)
    x = layers.BatchNormalization()(x)
    return tf.nn.relu(x)

def deformableConv(inp):
    out_p = convolution_block(inp, num_filters=inp.shape[-1]*3, kernel_size=1, dilation_rate=1)
    x = layers.MaxPooling2D(pool_size=(2,2))(out_p)
    x = layers.UpSampling2D(
        size=(2, 2), interpolation="bilinear")(x)
    x1 = layers.MaxPooling2D(pool_size=(8,8))(out_p)
    x1 = layers.UpSampling2D(
        size=(8, 8), interpolation="bilinear")(x1)
    x2 = layers.MaxPooling2D(pool_size=(4,4))(out_p)
    x2 = layers.UpSampling2D(
        size=(4, 4), interpolation="bilinear")(x2)
    x4 = layers.MaxPooling2D(pool_size=(12,12))(out_p)
    x4 = layers.UpSampling2D(
        size=(12, 12), interpolation="bilinear")(x4)
    
    x11 = convolution_block(x, num_filters=inp.shape[-1], kernel_size=1, dilation_rate=1)
    x12 = convolution_block(x1, num_filters=inp.shape[-1], kernel_size=1, dilation_rate=1)
    x13 = convolution_block(x2, num_filters=inp.shape[-1], kernel_size=1, dilation_rate=1)
    x15 = convolution_block(x4, num_filters=inp.shape[-1], kernel_size=1, dilation_rate=1)
    conc = layers.Concatenate(axis=-1)([x11,x12,x13,x15])
    return conc

def DilatedSpatialPyramidPooling(dspp_input):
    dims = dspp_input.shape
    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)
    x = convolution_block(x, kernel_size=1, use_bias=True)
    out_pool = layers.UpSampling2D(
        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation="bilinear",
    )(x)
    conc = deformableConv(dspp_input)
    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)
    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)
    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)
    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)
    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18, conc])
    output = convolution_block(x, kernel_size=1)
    return output

def DeeplabV3Plus(image_size, num_classes):
    model_input = keras.Input(shape=(image_size, image_size, 3))
    resnet101 = tf.keras.applications.ResNet101(
        weights="imagenet", include_top=False, input_tensor=model_input
    )
    x1 = resnet101.get_layer("conv4_block23_2_relu").output
    input_b = resnet50.get_layer("conv2_block3_2_relu").output
    x = DilatedSpatialPyramidPooling(x1)
    
    input_a = layers.UpSampling2D(
        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),
        interpolation="bilinear",
    )(x)
    
    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)
    x = layers.Concatenate(axis=-1)([input_a, input_b])

    x = convolution_block(x)
    x = layers.UpSampling2D(
        size=(image_size // x.shape[1], image_size // x.shape[2]),
        interpolation="bilinear",
    )(x)
    
    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding="same", activation='softmax')(x)
    
    return keras.Model(inputs=model_input, outputs=model_output)


_MIN_SCALE = 0.5
_MAX_SCALE = 2.0
_IGNORE_LABEL = 255
IMAGE_SIZE = 384

def get_filenames(is_training, data_dir):
  """Return a list of filenames.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: path to the the directory containing the input data.
  Returns:
    A list of file names.
  """
  if is_training:
    return [os.path.join(data_dir, 'voc_train.record')]
  else:
    return [os.path.join(data_dir, 'voc_val.record')]

#function to parse records from TFRecord format
def parse_record(raw_record):
  """Parse PASCAL image and label from a tf record."""
  keys_to_features = {
      'image/height':
      tf.io.FixedLenFeature((), tf.int64),
      'image/width':
      tf.io.FixedLenFeature((), tf.int64),
      'image/encoded':
      tf.io.FixedLenFeature((), tf.string, default_value=''),
      'image/format':
      tf.io.FixedLenFeature((), tf.string, default_value='jpeg'),
      'label/encoded':
      tf.io.FixedLenFeature((), tf.string, default_value=''),
      'label/format':
      tf.io.FixedLenFeature((), tf.string, default_value='png'),
  }

  parsed = tf.io.parse_single_example(raw_record, keys_to_features)

  image = tf.io.decode_image(
      tf.reshape(parsed['image/encoded'], shape=[]), 3)
  image = tf.cast(tf.image.convert_image_dtype(image, dtype=tf.uint8),dtype=tf.float32)

  image.set_shape([None, None, 3])

  label = tf.image.decode_png(
      tf.reshape(parsed['label/encoded'], shape=[]), 1)
  label = tf.cast(tf.image.convert_image_dtype(label, dtype=tf.uint8),dtype=tf.int32)
  label.set_shape([None, None, 1])

  return image, label

#function for processing and augmenting the dataset
def preprocess_image(image, label, is_training):
  """Preprocess a single image of layout [height, width, depth]."""
  if is_training:
    # Randomly scale the image and label.
    image, label = preprocessing.random_rescale_image_and_label(
        image, label, _MIN_SCALE, _MAX_SCALE)

    # Randomly crop or pad a [_HEIGHT, _WIDTH] section of the image and label.
    image, label = preprocessing.random_crop_or_pad_image_and_label(
        image, label, IMAGE_SIZE, IMAGE_SIZE, _IGNORE_LABEL)

    # Randomly flip the image and label horizontally.
    image, label = preprocessing.random_flip_left_right_image_and_label(
        image, label)
    
    
    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])
    label.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])
    
  image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])
  label = tf.image.resize(images=label, size=[IMAGE_SIZE, IMAGE_SIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    
  image = preprocessing.mean_image_subtraction(image)
  label = tf.cast(label, tf.int32)
  label = tf.where(label==255, 0, label)
  return image, label

#function for preprocessing and creating batches
def input_fn(is_training, data_dir, batch_size, model, num_epochs=1):
  """Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
  Args:
    is_training: A boolean denoting whether the input is for training.
    data_dir: The directory containing the input data.
    batch_size: The number of samples per batch.
    num_epochs: The number of epochs to repeat the dataset.
  Returns:
    A tuple of images and labels.
  """
  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, '/content/drive/MyDrive'))
  dataset = dataset.flat_map(tf.data.TFRecordDataset)

  if is_training:
    # When choosing shuffle buffer sizes, larger sizes result in better
    # randomness, while smaller sizes have better performance.
    # is a relatively small dataset, we choose to shuffle the full epoch.
    dataset = dataset.shuffle(buffer_size=10582)

  dataset = dataset.map(parse_record)
  dataset = dataset.map(
          lambda image, label: preprocess_image(image, label, is_training))

    # We call repeat after shuffling, rather than before, to prevent separate
    # epochs from blending together.

  dataset = dataset.batch(batch_size,drop_remainder=True)
  dataset = dataset.repeat()
  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

  return dataset

optim = tf.keras.optimizers.Adam(0.0001)
metrics = [UpdatedMeanIoU(num_classes=21),'sparse_categorical_accuracy']

loss = keras.losses.SparseCategoricalCrossentropy()

model = DeeplabV3Plus(image_size=384, num_classes=21)
learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(
    0.006,
    decay_steps=54000,
    end_learning_rate=0.000003,
    power=0.9)
opt = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)

metrics2 = ['categorical_accuracy']
model.compile(
    optimizer=opt,
    loss=loss,
    metrics=metrics,
    run_eagerly=True
)

alpha = 0.0001  # weight decay coefficient

checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath="/content/drive/MyDrive/mlp_cw4/Deformable.h5",
    monitor= "val_updated_mean_io_u",
    mode='max',
    save_best_only=True)

for layer in model.layers:
    if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):
        layer.add_loss(add_l2_regularization_kernel(layer, alpha))
    if hasattr(layer, 'bias_regularizer') and layer.use_bias:
        layer.add_loss(add_l2_regularization_bias(layer, alpha))


for i in range(36):
    train_dataset = input_fn(True,'/content/drive/MyDrive', 8, model61, 4)
    val_set = input_fn(False,'/content/drive/MyDrive', 8, model61, 4)

    model.fit(train_dataset, validation_data=val_set, epochs=1058,steps_per_epoch=1,validation_steps=1,verbose=2, callbacks=[checkpoint])